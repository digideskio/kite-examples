# Kite End-to-End Demo

This example demonstrates a solution to a fairly common problem: understanding overall
behavior from isolated events. In this example, an event is a single request to
a web application, and what we want to find is how long, on average, users spend
interacting with the web site. To do this, raw events are sent as logs to Hadoop as
they happen, so the web app doesn't do any extra work. Then, an hourly job groups the
log events into sessions, a group of requests from the same person, and stores a
summary of each session in another dataset.

Requests to the web app are logged using log4j, which is configured to send events to
Hadoop through Flume. Flume stores the events in an `events` dataset, which is the
input to a Crunch job that groups them into sessions. This job is run hourly by Oozie.
Session summaries are then stored in the `sessions` dataset that can be queried with
SQL through Impala to characterize behavior, like the average duration of a session.

If you run into trouble, check out the [Troubleshooting section](../README.md#troubleshooting).

## Getting started

1. This example assumes that you have VirtualBox or VMWare installed and have a
   running [Cloudera QuickStart VM][getvm]. See the
   [Getting Started](../README.md#getting-started) and
   [Troubleshooting](../README.md#troubleshooting) sections for help.
2. In that VM, check out a copy of this demo so you can build the code and
   follow along:
  * Open "Applications" > "System Tools" > "Terminal"
  * Then run:

```bash
git clone https://github.com/kite-sdk/kite-examples.git
cd kite-examples
cd demo
```

If you are using a prepared Kite VM, the `git clone` command is already done for you.

[getvm]: https://ccp.cloudera.com/display/SUPPORT/Cloudera+QuickStart+VM

## Configuring the VM

If you are using a prepared Kite VM, these configuration steps are already done for you.

### __Enable Flume user impersonation__
Flume needs to be able to impersonate the owner
 of the dataset it is writing to. (This is like Unix `sudo`, see
[Configuring Flume's Security Properties](http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/latest/CDH4-Security-Guide/cdh4sg_topic_4_2.html)
for further information.) This is already configured for Cloudera Manager 5 onwards.
For earlier versions, in the Cloudera Manager web interface,
* __Update the configuration__
  * Click on the "hdfs1" service in [CM services](http://localhost:7180/cmf/services/status)
  * Under the "Configuration" drop-down, select "View and Edit"
  * Search for "valve"
  * Add the following XML snippet as the "Cluster-wide Configuration Safety Valve for core-site.xml" and click "Save Changes"

```
<property>
  <name>hadoop.proxyuser.flume.groups</name>
  <value>*</value>
</property>
<property>
  <name>hadoop.proxyuser.flume.hosts</name>
  <value>*</value>
</property>
```

### __Configure the flume agent__

* __Update the Flume agent configuration__
  * Click on the "Cloudera Manager" logo in the upper-left corner of CM, which
    takes you to [CM services](http://localhost:7180/cmf/services/status)
  * Click on the "flume1" service
  * Under the "Configuration" drop-down, select "View and Edit"
  * Search for "Configuration file"
  * Replace the contents of of the "Configuration file" with the
    `flume.properties` file from this repository
  * Click "Save Changes"
* __(Re)Start the Flume agent__
  * Go back to the "flume1" service
  * Under the "Actions" drop-down on the right side, select "Restart"

    If you are running this example from you machine and not from a QuickStart VM login,
then make sure you change the value of the `proxyUser` setting in the agent
configuration to the user that you are logged in as. Save changes,
then start the Flume agent.

Next: __Ensure Oozie is running__ From Cloudera Manager, start the Oozie service.

## Building

To build the project, type

```bash
mvn install
```

This creates the following artifacts:

* a JAR file with data classes generated from avro schemas `standard_event.avsc` and `session.avsc` (in `demo-core`)
* a WAR file for the webapp that logs application events, the raw data to analyze (in `demo-logging-webapp`)
* a JAR file for running the Crunch job to summarize events as sessions (in
`demo-crunch`)
* an Oozie application for running the Crunch job on a periodic basis (in `demo-oozie`)
* a WAR file for the webapp that displays reports generated by Impala (in
`demo-reports-webapp`)

## Running

### Create the datasets

First we need to create the datasets: one called `events` for the raw events,
and `sessions` for the derived sessions.

Schemas for both datasets are provided in the `demo-core/src/main/avro/` directory. It
is a good practice to store schemas in HDFS, so that applications can refer to the
schema by URI rather than passing the schema text. To copy the schemas into HDFS, run
these commands:

```bash
hadoop fs -mkdir schemas
hadoop fs -copyFromLocal demo-core/src/main/avro/*.avsc schemas
```

Next, we will create datasets using the `dataset` command, Kite's command-line
interface. If you don't already have a copy, you can download it using the
instructions in the [CLI tutorial](http://kitesdk.org/docs/current/usingkiteclicreatedataset.html#Preparation).

If these datasets already exist, you can delete them with the following commands:

```bash
./dataset delete events --directory /tmp/data
./dataset delete sessions --directory /tmp/data
```

To create the `sessions` dataset, run the `dataset create` command, passing in
the session schema in HDFS.

```bash
./dataset create sessions --schema hdfs:/user/$USER/schemas/session.avsc \
                          --directory /tmp/data
```

Before creating the events dataset, we need to create a second configuration file
that configures how Kite stores the data: a *partition strategy*. We want to
store events so that each hour of logs can be processed separately, without
needing to read the entire dataset. This is important because we expect the
events table to be really large, so reading the entire table every hour won't be
possible.

To configure Kite, we need to generate a partitioning configuration file. We
will use the `dataset partition-config` command and pass `field:type` pairs to
add a partitions to the configuration. The command verifies that the partitioning
strategy is consistent with the given schema. For example, a year can only be
extracted from a timestamp (long).

```bash
./dataset partition-config --schema hdfs:/user/$USER/schemas/standard_event.avsc \
    timestamp:year timestamp:month timestamp:day timestamp:hour timestamp:minute \
    --output year-month-day-hour-min.json
```

Now we can create the `events` dataset, passing in the event schema and the
partition strategy config file.

```bash
./dataset create events --schema hdfs:/user/$USER/schemas/standard_event.avsc \
                        --partition-by year-month-day-hour-min.json \
                        --directory /tmp/data
```

Now the datasets are ready! But before we move on, there are a couple more
things to know. The `dataset` command can also be used to show records in a
dataset and can inspect or delete datasets. The `dataset` command will also
describe commands, options, and show examples by using `dataset help`.

### Create events

Next we can run the webapps. They can be used in a Java EE 6 servlet
container; for this example we'll start an embedded Tomcat instance using Maven:

```bash
mvn tomcat7:run
```

This will run both the logging app and the reports app we will use later, so leave
the command running in a separate window.

Navigate to [http://localhost:8034/demo-logging-webapp/](http://localhost:8034/demo-logging-webapp/),
which presents you with a very simple web form that sends messages. Each time this
form is submitted, the application logs the request as an event.

The events are sent through log4j to the Flume agent, and the agent writes the events
with the HDFS file sink.

Rather than creating lots of events manually, it's easier to simulate two users with
a script as follows:

```bash
./bin/simulate-activity.sh 1 10 > /dev/null &
./bin/simulate-activity.sh 2 10 > /dev/null &
```

After about 30 seconds, you should be able to see event files in the
[filesystem](http://localhost:8888/filebrowser/#/tmp/data/events) organized by year,
month, etc. You can also view the events with the `dataset show` command.

```bash
./dataset show events --directory /tmp/data
```

### Generate the derived sessions

Wait about 30 seconds to make sure Flume has flushed the events to the
[filesystem](http://localhost:8888/filebrowser/#/tmp/data/events),
then run the Crunch job to generate derived session data from the events:

```bash
(cd demo-crunch; mvn kite:run-tool)
```

Notice that we are using maven to run the Crunch job. We've used maven to build and
package the job, so the maven pom has information with all of the job's dependencies.
The `kite:run-tool` goal takes advantage of that information and submits the job with
its dependencies automatically configured. It will execute the `run` method of the
`Tool`, in this case `CreateSessions`, which launches the Crunch job on the cluster.

The `Tool` class to run, as well as the cluster settings, are found from the configuration
of the `kite-maven-plugin` in `demo-crunch/pom.xml`.

When it's complete you should see a new data file in [`/tmp/data/sessions`]
(http://localhost:8888/filebrowser/#/tmp/data/sessions).

### Run session analysis

The `sessions` dataset is now populated with data, but we need to tell Impala to
refresh its metastore so the new `sessions` table will be visible:

```bash
impala-shell -q 'invalidate metadata'
```

One way to explore the results is by using the `demo-reports-webapp` running at
[http://localhost:8034/demo-reports-webapp/](http://localhost:8034/demo-reports-webapp/),
which uses JDBC to run Impala queries for a few pre-defined reports. (Note this only
work with Impala 1.1 or later, see instructions above.)

Another way is to run ad hoc SQL queries using the Hue interfaces to
[Impala](http://localhost:8888/impala/) or [Hive](http://localhost:8888/beeswax/).
Here are some queries to try out:

```
DESCRIBE sessions
```

```
SELECT * FROM sessions
```

```
SELECT AVG(duration) FROM sessions
```

### Use Oozie to create derived sessions periodically

Oozie is a workflow management system for running jobs on a Hadoop cluster. Rather than
launching jobs from the developer console, Oozie applications are deployed to the
cluster then launched from there.

A quick note on terminology: an Oozie _application_ is all the packaged code and
configuration. There are three types of Oozie application: workflow applications that
describe a workflow of actions; coordinator applications that run workflows based on
time and data triggers; and bundle applications that run batches of coordinator
applications. An Oozie _job_ is the running instantiation of an Oozie _application_.

The `kite-maven-plugin` provides Maven goals for packaging, deploying,
and running Oozie applications.

Oozie applications are stored in HDFS to allow Oozie to access and run them, so
the first thing we do is deploy the Oozie application to HDFS.

```bash
cd demo-oozie
mvn kite:deploy-app
```

The filesystem to deploy to is specified by the `deployFileSystem` setting for the
`kite-maven-plugin`. By default, Oozie applications are stored in the
`/user/<user>/apps` directory on
HDFS. You can navigate to this location using the
[web interface](http://localhost:8888/filebrowser/#/user) to see if the
application has been successfully deployed.

Before running an Oozie coordinator application, let's run a one-off workflow. The
Oozie server to use is specified by `oozieUrl` in the plugin configuration.

```bash
mvn kite:run-app -Dkite.applicationType=workflow
```

Monitor the workflow job using the [Oozie application in Hue](http://localhost:8888/oozie/list_oozie_workflows/).
You can click through to see the underlying MapReduce jobs (just one in this case)
that are run by Crunch.

Next, let's run a coordinator application that runs the workflow application once
every minute. Build the coordinator version of the app:

```bash
mvn package kite:deploy-app -Dkite.applicationType=coordinator
```

We need to create events continuously, which we do by running the
user simulation script again. This time we don't specify a limit on the number
of events to create, so it runs indefinitely.

```bash
./bin/simulate-activity.sh 1
```

Now we can run the Oozie coordinator application.

```bash
mvn kite:run-app -Dkite.applicationType=coordinator -Dstart="$(date -u +"%Y-%m-%dT%H:%MZ")"
```

Monitor the coordinator and resulting workflow jobs using the [Oozie application in Hue](http://localhost:8888/oozie/list_oozie_coordinators).

After a minute or two when a workflow job has completed, you should see new files appear
in the `sessions` dataset, in
[`/tmp/data/sessions`](http://localhost:8888/filebrowser#/tmp/data/sessions).
When you see new files appear, then try running the session analysis from above.

When you have finished, stop the user simulation script by killing the process
(with Ctrl-C). Kill the Oozie coordinator job through the Hue web interface.

